{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1da78e3-b4c4-43d3-8a88-10bdef4fdc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This might take a few minutes depending on size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Welcome\\AppData\\Local\\Temp\\ipykernel_17856\\1065861411.py:10: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 7079081 rows and 35 columns.\n",
      "\n",
      "Columns with missing values:\n",
      " op_carrier_fl_num            1\n",
      "dep_time                 92659\n",
      "dep_delay                92970\n",
      "taxi_out                 95734\n",
      "wheels_off               95734\n",
      "wheels_on                97856\n",
      "taxi_in                  97856\n",
      "arr_time                 97854\n",
      "arr_delay               113814\n",
      "cancellation_code      6982766\n",
      "crs_elapsed_time             1\n",
      "actual_elapsed_time     113814\n",
      "air_time                113814\n",
      "dtype: int64 \n",
      "\n",
      "Missing values handled.\n",
      "\n",
      "Binary target variable 'is_delayed' created.\n",
      "\n",
      "Delay distribution:\n",
      "is_delayed\n",
      "0    79.851627\n",
      "1    20.148373\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Selected features and target variable.\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.25 GiB for an array with shape (347, 6965267) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSelected features and target variable.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# ===========================\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Step 6: One-hot encoding of categorical variables\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# ===========================\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Convert categorical features to dummy/indicator variables for ML models\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m X_encoded = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mop_unique_carrier\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43morigin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeatures after encoding have shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_encoded.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# ===========================\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Step 7: Save processed data for modeling\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# ===========================\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Save the clean and processed feature matrix and the target variable to CSVs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:225\u001b[39m, in \u001b[36mget_dummies\u001b[39m\u001b[34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[39m\n\u001b[32m    215\u001b[39m         dummy = _get_dummies_1d(\n\u001b[32m    216\u001b[39m             col[\u001b[32m1\u001b[39m],\n\u001b[32m    217\u001b[39m             prefix=pre,\n\u001b[32m   (...)\u001b[39m\u001b[32m    222\u001b[39m             dtype=dtype,\n\u001b[32m    223\u001b[39m         )\n\u001b[32m    224\u001b[39m         with_dummies.append(dummy)\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     result = \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwith_dummies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    227\u001b[39m     result = _get_dummies_1d(\n\u001b[32m    228\u001b[39m         data,\n\u001b[32m    229\u001b[39m         prefix,\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m         dtype=dtype,\n\u001b[32m    235\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m concat_axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     mgrs = \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[32m0\u001b[39m].concat_horizontal(mgrs, axes)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].nblocks > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[39m, in \u001b[36m_maybe_reindex_columns_na_proxy\u001b[39m\u001b[34m(axes, mgrs_indexers, needs_copy)\u001b[39m\n\u001b[32m    220\u001b[39m         mgr = mgr.reindex_indexer(\n\u001b[32m    221\u001b[39m             axes[i],\n\u001b[32m    222\u001b[39m             indexers[i],\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m             use_na_proxy=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[32m    228\u001b[39m         )\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         mgr = \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     new_mgrs.append(mgr)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    591\u001b[39m         new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcopy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m res.axes = new_axes\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    597\u001b[39m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:822\u001b[39m, in \u001b[36mBlock.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    820\u001b[39m refs: BlockValuesRefs | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     values = \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m     refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 2.25 GiB for an array with shape (347, 6965267) and data type bool"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "\n",
    "\n",
    "file_path = r'C:\\Users\\Welcome\\OneDrive - NSBM\\Desktop\\3rd_year\\flight delay\\flight_data_2024.csv'\n",
    "\n",
    "print(\"Loading dataset. This might take a few minutes depending on size...\")\n",
    "data = pd.read_csv(file_path)\n",
    "print(f\"Dataset loaded successfully with {len(data)} rows and {len(data.columns)} columns.\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 2: Inspect missing values\n",
    "# ===========================\n",
    "# In real-world datasets, missing data is common and must be handled before modeling.\n",
    "missing_counts = data.isnull().sum()\n",
    "print(\"Columns with missing values:\\n\", missing_counts[missing_counts > 0], \"\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 3: Handle missing values\n",
    "# ===========================\n",
    "# For 'arr_delay' and related critical features, drop rows where missing as target or important input is unknown\n",
    "data = data.dropna(subset=['arr_delay'])\n",
    "\n",
    "# For delay cause columns, fill missing with 0, which means no delay from that reason\n",
    "delay_cause_cols = ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "for col in delay_cause_cols:\n",
    "    data[col] = data[col].fillna(0)\n",
    "\n",
    "# For timing columns like 'dep_time', fill missing values with median (simple imputation)\n",
    "timing_cols = ['dep_time', 'arr_time', 'actual_elapsed_time', 'air_time', 'taxi_out', 'taxi_in']\n",
    "for col in timing_cols:\n",
    "    data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "print(\"Missing values handled.\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 4: Create binary target variable 'is_delayed'\n",
    "# ===========================\n",
    "# Define a flight as delayed if actual arrival delay is more than 15 minutes\n",
    "data['is_delayed'] = (data['arr_delay'] > 15).astype(int)\n",
    "print(\"Binary target variable 'is_delayed' created.\\n\")\n",
    "print(f\"Delay distribution:\\n{data['is_delayed'].value_counts(normalize=True) * 100}\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 5: Select relevant features for modeling\n",
    "# ===========================\n",
    "# We choose features that can influence delay prediction based on domain knowledge\n",
    "features = ['month', 'day_of_week', 'op_unique_carrier', 'origin', 'dest', 'crs_dep_time', 'distance']\n",
    "X = data[features]\n",
    "y = data['is_delayed']\n",
    "\n",
    "print(\"Selected features and target variable.\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 6: One-hot encoding of categorical variables\n",
    "# ===========================\n",
    "# Convert categorical features to dummy/indicator variables for ML models\n",
    "X_encoded = pd.get_dummies(X, columns=['op_unique_carrier', 'origin', 'dest'], drop_first=True)\n",
    "\n",
    "print(f\"Features after encoding have shape: {X_encoded.shape}\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 7: Save processed data for modeling\n",
    "# ===========================\n",
    "# Save the clean and processed feature matrix and the target variable to CSVs\n",
    "X_encoded.to_csv('Data/processed_features.csv', index=False)\n",
    "y.to_csv('Data/target.csv', index=False)\n",
    "\n",
    "print(\"Processed features and target saved to 'data/' folder.\")\n",
    "print(\"You are now ready to move on to the modeling step!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4f1d3a-27b0-4a55-bcb1-b394e8c79757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba362b9-b082-46b3-953e-f35cf88fdaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'results' folder ensured at: C:\\Users\\Welcome\\OneDrive - NSBM\\Desktop\\3rd_year\\flight delay\\flight-delay-prediction-ml\\flight-delay-prediction-ml\\results\n",
      "\n",
      "Loading full dataset. This may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Welcome\\AppData\\Local\\Temp\\ipykernel_17856\\2283970607.py:28: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset loaded with 7079081 rows.\n",
      "\n",
      "Sampled 100000 rows for preprocessing.\n",
      "\n",
      "Missing values handled.\n",
      "\n",
      "Delay target created with distribution:\n",
      "is_delayed\n",
      "0    79.844276\n",
      "1    20.155724\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Selected feature columns and target variable.\n",
      "\n",
      "Categorical variables encoded using LabelEncoder.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Welcome\\AppData\\Local\\Temp\\ipykernel_17856\\2283970607.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = le.fit_transform(X[col])\n",
      "C:\\Users\\Welcome\\AppData\\Local\\Temp\\ipykernel_17856\\2283970607.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = le.fit_transform(X[col])\n",
      "C:\\Users\\Welcome\\AppData\\Local\\Temp\\ipykernel_17856\\2283970607.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = le.fit_transform(X[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed features saved to: C:\\Users\\Welcome\\OneDrive - NSBM\\Desktop\\3rd_year\\flight delay\\flight-delay-prediction-ml\\flight-delay-prediction-ml\\results\\processed_features_sample.csv\n",
      "Target saved to: C:\\Users\\Welcome\\OneDrive - NSBM\\Desktop\\3rd_year\\flight delay\\flight-delay-prediction-ml\\flight-delay-prediction-ml\\results\\target_sample.csv\n",
      "\n",
      "Preprocessing complete. You can now proceed to model training.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# ===========================\n",
    "# Define paths\n",
    "# ===========================\n",
    "\n",
    "# Your script folder (Notebooks), relative to project root\n",
    "script_folder = r'C:\\Users\\Welcome\\OneDrive - NSBM\\Desktop\\3rd_year\\flight delay\\flight-delay-prediction-ml\\flight-delay-prediction-ml\\Notebooks'\n",
    "\n",
    "# Define the path to the results folder (one level up from Notebooks, then 'results' folder)\n",
    "results_folder = os.path.join(os.path.dirname(script_folder), 'results')\n",
    "\n",
    "# Create the results folder if it doesn't exist\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "print(f\"'results' folder ensured at: {results_folder}\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 1: Load the full dataset\n",
    "# ===========================\n",
    "\n",
    "file_path = r'C:\\Users\\Welcome\\OneDrive - NSBM\\Desktop\\3rd_year\\flight delay\\flight_data_2024.csv'\n",
    "\n",
    "print(\"Loading full dataset. This may take a while...\")\n",
    "data = pd.read_csv(file_path)\n",
    "print(f\"Full dataset loaded with {len(data)} rows.\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 2: Sample the dataset\n",
    "# ===========================\n",
    "\n",
    "# Randomly sample 100,000 rows to reduce memory usage during processing\n",
    "data_sample = data.sample(100000, random_state=42)\n",
    "print(f\"Sampled {len(data_sample)} rows for preprocessing.\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 3: Handle missing values\n",
    "# ===========================\n",
    "\n",
    "# Drop rows with missing 'arr_delay' since it's your target variable\n",
    "data_sample = data_sample.dropna(subset=['arr_delay'])\n",
    "\n",
    "# Fill NaNs in delay reason columns with 0 (means no delay from that cause)\n",
    "delay_cols = ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay']\n",
    "for col in delay_cols:\n",
    "    data_sample[col] = data_sample[col].fillna(0)\n",
    "\n",
    "# Fill missing timing columns with median to maintain consistency\n",
    "timing_cols = ['dep_time', 'arr_time', 'actual_elapsed_time', 'air_time', 'taxi_out', 'taxi_in']\n",
    "for col in timing_cols:\n",
    "    data_sample[col] = data_sample[col].fillna(data_sample[col].median())\n",
    "\n",
    "print(\"Missing values handled.\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 4: Create binary delay target\n",
    "# ===========================\n",
    "\n",
    "data_sample['is_delayed'] = (data_sample['arr_delay'] > 15).astype(int)\n",
    "print(f\"Delay target created with distribution:\\n{data_sample['is_delayed'].value_counts(normalize=True) * 100}\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 5: Select features and target\n",
    "# ===========================\n",
    "\n",
    "features = ['month', 'day_of_week', 'op_unique_carrier', 'origin', 'dest', 'crs_dep_time', 'distance']\n",
    "X = data_sample[features]\n",
    "y = data_sample['is_delayed']\n",
    "\n",
    "print(\"Selected feature columns and target variable.\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 6: Label encode categorical variables\n",
    "# ===========================\n",
    "\n",
    "categorical_cols = ['op_unique_carrier', 'origin', 'dest']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Categorical variables encoded using LabelEncoder.\\n\")\n",
    "\n",
    "# ===========================\n",
    "# Step 7: Save processed data for modeling\n",
    "# ===========================\n",
    "\n",
    "# Define full save paths\n",
    "features_save_path = os.path.join(results_folder, 'processed_features_sample.csv')\n",
    "target_save_path = os.path.join(results_folder, 'target_sample.csv')\n",
    "\n",
    "X.to_csv(features_save_path, index=False)\n",
    "y.to_csv(target_save_path, index=False)\n",
    "\n",
    "print(f\"Processed features saved to: {features_save_path}\")\n",
    "print(f\"Target saved to: {target_save_path}\\n\")\n",
    "\n",
    "print(\"Preprocessing complete. You can now proceed to model training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99121e47-3f31-4251-a97f-1d9777f6bfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
